{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE6250BDH Deep Learning Labs\n",
    "## 0. Introduction to PyTorch\n",
    "\n",
    "In this chapter, we will learn basic usage of PyTorch.\n",
    "There are many good tutorials on PyTorch on web.\n",
    "We highly recommend you to follow the official [tutorial](http://pytorch.org/tutorials/) even though this tutorial is also mainly from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing PyTorch, you can import `torch` in Python to use PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is very similar with Numpy as they say it is a replacement for Numpy to use the power of GPUs. Although there are still missing components, it has many same/similar functions for constructing or manipulating 'Tensor's.\n",
    "\n",
    "A basic object used in PyTorch is 'Tensor' which is equivalent to 'ndarray' in Numpy. Similarly to Numpy, there are multiple types of Tensors, e.g. Float, Double, Int, Long, etc. Most of time, however, we will use FloatTensor mainly (and it is a default type for the most of functions) to utilize GPU and LongTensor sometime for target/label values.\n",
    "\n",
    "Lets try to create a Tensor. If you call `torch.Tensor(rows, cols)`, it will return a FloatTensor without initialization (with garbage values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 6.8555e+03  4.5661e-41  6.8555e+03\n",
       " 4.5661e-41         nan  4.5661e-41\n",
       " 4.4721e+21  1.6647e-41  6.7262e-44\n",
       " 0.0000e+00  6.7262e-44  0.0000e+00\n",
       " 0.0000e+00  0.0000e+00  0.0000e+00\n",
       "[torch.FloatTensor of size 5x3]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor(5, 3) # same result with torch.FloatTensor(5,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create initialized Tensor filled with 1s, 0s, or random numbers from a uniform distribtution by using `torch.ones`, `torch.zeros`, or `torch.rand` repectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1  1  1\n",
      " 1  1  1\n",
      " 1  1  1\n",
      " 1  1  1\n",
      " 1  1  1\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "\n",
      " 0  0  0\n",
      " 0  0  0\n",
      " 0  0  0\n",
      " 0  0  0\n",
      " 0  0  0\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "\n",
      " 0.6307  0.4901  0.4400\n",
      " 0.0130  0.4933  0.4533\n",
      " 0.2542  0.7716  0.5080\n",
      " 0.4358  0.2828  0.4932\n",
      " 0.1586  0.9872  0.2415\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones(5,3)\n",
    "print(x_ones)\n",
    "\n",
    "x_zeros = torch.zeros(5,3)\n",
    "print(x_zeros)\n",
    "\n",
    "x_uniform = torch.rand(5,3)\n",
    "print(x_uniform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Try `torch.eye`, `torch.linspace`, `torch.logspace`, etc.\n",
    "### Exercise: Try other random functions from [here](http://pytorch.org/docs/master/torch.html#random-sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting from/to Numpy ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create a Tensor from Numpy ndarray or vice versa. In fact, we may do this many times in a project since we want to utilize many Numpy-based libraries (e.g., Pandas, Scikit-learn, Matplotlib, etc.) as well as GPU computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can simply call `torch.from_numpy(ndarray)` to create a `Tensor` from a `numpy.ndarray`. **Be careful that the returned Tensor and original ndarray share the same memory**. Therefore, if you modify the Tensor, it will be reflected in the ndarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  2.  3.]\n",
      "\n",
      " 1\n",
      " 2\n",
      " 3\n",
      "[torch.DoubleTensor of size 3]\n",
      "\n",
      "[-1.  2.  3.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np_array = np.array([1., 2., 3.])\n",
    "print(np_array)\n",
    "torch_tensor = torch.from_numpy(np_array)\n",
    "print(torch_tensor)\n",
    "\n",
    "# Modify the Tensor\n",
    "torch_tensor[0] = -1.0\n",
    "print(np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the reverse way of conversion, you can call `numpy()` on a Tensor. Again, resulting ndarray shares the memory with the Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.0285\n",
      " 0.7040\n",
      " 0.7574\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "[ 0.02854237  0.70398551  0.75743389]\n",
      "\n",
      " 0.0571\n",
      " 0.7040\n",
      " 0.7574\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "another_torch_tensor = torch.rand(3)\n",
    "print(another_torch_tensor)\n",
    "another_np_array = another_torch_tensor.numpy()\n",
    "print(another_np_array)\n",
    "\n",
    "# Modify ndarray\n",
    "another_np_array[0] *= 2.0\n",
    "print(another_torch_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use standard numpy-like indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.2528  0.6713  0.6938\n",
      " 0.8498  0.7871  0.2768\n",
      " 0.4162  0.8565  0.6468\n",
      "[torch.FloatTensor of size 3x3]\n",
      "\n",
      "\n",
      " 0.6713\n",
      " 0.7871\n",
      " 0.8565\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "\n",
      " 0.2528  0.6713  0.6938\n",
      " 0.8498  0.7871  0.2768\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "A = torch.rand(3,3)\n",
    "print(A)\n",
    "print(A[:, 1])\n",
    "print(A[:2, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arithmetic Operations\n",
    "Arithmetic operations with `+-*/` operators are all element-wise computation. Therefore, if you want to do some matrix computations such as matrix-matrix (or vector) multiplication, you need to call separate functions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.4530  1.0410  0.8313\n",
      " 1.5518  1.7824  0.8284\n",
      " 0.4190  1.3785  0.9298\n",
      "[torch.FloatTensor of size 3x3]\n",
      "\n",
      "\n",
      " 0.0506  0.2482  0.0953\n",
      " 0.5966  0.7834  0.1527\n",
      " 0.0012  0.4471  0.1830\n",
      "[torch.FloatTensor of size 3x3]\n",
      "\n",
      "\n",
      " 0.0506  0.2482  0.0953\n",
      " 0.5966  0.7834  0.1527\n",
      " 0.0012  0.4471  0.1830\n",
      "[torch.FloatTensor of size 3x3]\n",
      "\n",
      "\n",
      " 0.5238  1.1238  0.6014\n",
      " 0.7234  1.2420  0.6292\n",
      " 0.6864  1.3440  0.7126\n",
      "[torch.FloatTensor of size 3x3]\n",
      "\n",
      "\n",
      " 1.1238\n",
      " 1.2420\n",
      " 1.3440\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "B = torch.rand(3,3)\n",
    "print(A+B)\n",
    "print(A*B)\n",
    "# Another elementwise multiplication\n",
    "print(torch.mul(A,B))\n",
    "\n",
    "# Matrix-Matrix multiplication\n",
    "print(torch.mm(A,B))\n",
    "# Matrix-Vector multiplication\n",
    "print(torch.mv(A,B[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many predefined operations for your convenience such as batch multiplication with addition, etc. Please read [PyTorch Docs](http://pytorch.org/docs/master/torch.html#math-operations) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have NVIDIA GPU(s), we can accelerate computation once we move Tensors onto GPU.\n",
    "Let's compare how much GPU can accelerate especially matrix operations.\n",
    "We will do a matrix-matrix multiplication between two 10k-by-10k matrices on both CPU and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 2.2858e-01  1.3732e-02  8.2131e-01  ...   2.2927e-02  5.8620e-01  1.5310e-01\n",
       " 9.2715e-01  7.5609e-01  2.5542e-01  ...   6.5381e-01  9.6541e-01  3.5529e-02\n",
       " 2.4621e-01  2.2214e-01  2.8611e-01  ...   6.7490e-01  1.6900e-02  1.6574e-01\n",
       "                ...                   ⋱                   ...                \n",
       " 3.9399e-01  5.7185e-01  9.1783e-01  ...   1.9630e-01  6.7986e-01  4.7228e-01\n",
       " 2.4352e-01  7.6901e-01  4.5620e-01  ...   2.7947e-01  9.9385e-01  8.1377e-01\n",
       " 5.9080e-01  2.7172e-01  7.7285e-02  ...   9.7306e-02  1.1646e-01  4.6459e-02\n",
       "[torch.FloatTensor of size 10000x10000]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_cpu = torch.rand(10000, 10000)\n",
    "mat_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.3 s, sys: 21.1 s, total: 1min\n",
      "Wall time: 1.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       " 3365.9849  2519.5752  2503.7212  ...   2508.4438  2507.6111  2514.6704\n",
       " 2519.5752  3367.2944  2488.5889  ...   2517.9202  2500.6267  2534.6584\n",
       " 2503.7212  2488.5889  3302.7214  ...   2491.5786  2499.0010  2510.0322\n",
       "              ...                  ⋱                 ...               \n",
       " 2508.4438  2517.9202  2491.5786  ...   3344.3386  2496.6680  2525.6689\n",
       " 2507.6111  2500.6267  2499.0010  ...   2496.6680  3313.3633  2518.6372\n",
       " 2514.6704  2534.6584  2510.0322  ...   2525.6689  2518.6372  3389.2395\n",
       "[torch.FloatTensor of size 10000x10000]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "torch.mm(mat_cpu.t(), mat_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need a GPU for this comparison\n",
    "We can check its availability like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    cuda = True\n",
    "else:\n",
    "    cuda = False\n",
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 2.2974e-01  2.6620e-01  3.0774e-01  ...   8.8185e-01  1.0511e-01  1.8076e-01\n",
       " 5.3319e-02  3.5115e-01  3.3239e-01  ...   6.6390e-01  6.8324e-01  7.8843e-01\n",
       " 7.3126e-01  5.0033e-01  2.0959e-01  ...   1.9366e-01  4.4930e-01  1.7191e-01\n",
       "                ...                   ⋱                   ...                \n",
       " 5.0444e-01  2.5376e-01  9.5575e-01  ...   6.3547e-01  4.8272e-01  8.0048e-01\n",
       " 3.2083e-01  7.2077e-01  1.3644e-01  ...   3.9801e-01  6.1456e-01  8.2535e-01\n",
       " 8.3446e-01  4.5157e-01  9.5542e-01  ...   5.0632e-01  1.3479e-01  3.2355e-01\n",
       "[torch.cuda.FloatTensor of size 10000x10000 (GPU 0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_gpu = torch.rand(10000, 10000)\n",
    "if cuda:\n",
    "    mat_gpu = mat_gpu.cuda()\n",
    "mat_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 172 ms, sys: 96 ms, total: 268 ms\n",
      "Wall time: 269 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       " 3354.0352  2486.5952  2490.0027  ...   2493.8403  2553.8179  2494.9255\n",
       " 2486.5952  3312.3586  2477.0583  ...   2499.0500  2518.4255  2484.4612\n",
       " 2490.0027  2477.0583  3303.1865  ...   2476.4111  2505.9092  2464.9626\n",
       "              ...                  ⋱                 ...               \n",
       " 2493.8403  2499.0500  2476.4111  ...   3328.9001  2540.3438  2489.8201\n",
       " 2553.8179  2518.4255  2505.9092  ...   2540.3438  3408.5183  2530.0640\n",
       " 2494.9255  2484.4612  2464.9626  ...   2489.8201  2530.0640  3285.8259\n",
       "[torch.cuda.FloatTensor of size 10000x10000 (GPU 0)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "torch.mm(mat_gpu.t(), mat_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you see the speed-up? It will be much critical if we use larger matrices, more matrix computations, and a deeper neural network model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable\n",
    "PyTorch provide a functionality of automatic differentiation with a package `autograd` and Variable is the key class for utilizing it.\n",
    "\n",
    "Variable wraps a Tensor as its data and maintain another Tensor for the gradient with respect to this data Tensor. Also, almost all of built-in operations in PyTorch supports automatic differentiation with Variable. Therefore, we can call `.backward()` on a computation graph, e.g. neural network, after we finish our computation on the graph, then we can get automatically accumulated gradient for each Variable related with the graph.\n",
    "\n",
    "Let's try a simple example for easier understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.5000\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.1000\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1.1000\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "# Create some Tensors and a Variable\n",
    "x = Variable(torch.FloatTensor([2.0]), requires_grad=False)\n",
    "w = Variable(torch.FloatTensor([0.5]), requires_grad=True)\n",
    "b = Variable(torch.FloatTensor([0.1]), requires_grad=True)\n",
    "print(x)\n",
    "print(w)\n",
    "print(b)\n",
    "\n",
    "# Define a computational graph\n",
    "y = w*x + b # Currently, y = 0.5x + 0.1 and y(2) = 1.1\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute gradients on the graph y and print the gradient w.r.t each Variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Variable containing:\n",
      " 2\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute gradients\n",
    "y.backward()\n",
    "\n",
    "print(x.grad)\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we set `requires_grad=False` for Variable `x`, it has `None` value.\n",
    "Also, if we do a simple math to differentiate it manually, we can easily get:\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial w} = \\frac{\\partial}{\\partial w}\\left(wx + b\\right) = x\\\\\n",
    "\\text{and}\\\\\n",
    "\\displaystyle \\frac{\\partial y}{\\partial w}\\Bigr|_{x=2} = 2 \n",
    "$$\n",
    "Similarly,\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial b} = \\frac{\\partial}{\\partial b}\\left(wx + b\\right) = 1\\\\\n",
    "\\text{and}\\\\\n",
    "\\displaystyle \\frac{\\partial y}{\\partial b}\\Bigr|_{x=2} = 1 \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the functionality of automatic differentiation, we can build a very complex computational graph such as a neural network with many layers without manually computing the gradients of parameters.\n",
    "\n",
    "Please refer to the official [tutorial](http://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html) for more details.\n",
    "\n",
    "In the next chapter, we will build a simple feed-forward neural network by using these components of PyTorch we have learnt."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
