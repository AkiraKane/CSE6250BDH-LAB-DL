{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE6250BDH Deep Learning Labs\n",
    "## 1. Feed-forward Neural Network\n",
    "\n",
    "In this chapter, we will learn how to implement a feed-forward neural network by using PyTorch.\n",
    "Before moving to neural networks, let's refresh the modeling with Scikit-learn that we have already done in the lab [Spark-mllib](http://www.sunlab.org/teaching/cse6250/fall2017/lab/spark-mllib/#Scikit-learn) and we will compare the results. If you have not completed that part, please complete it first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "X, y = load_svmlight_file(\"patients.svmlight\")\n",
    "X = X.toarray() # make it dense\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MaxAbsScaler().fit(X_train)\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "X_test_transformed = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform as sp_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  74 out of 100 | elapsed:    6.2s remaining:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    6.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "          fit_params={}, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fd84b14ce10>, 'kernel': ['linear', 'poly', 'rbf'], 'class_weight': [None, 'balanced']},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score=True, scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CV\n",
    "param_dist = {'C': sp_rand(0, 1000), 'kernel': ['linear', 'poly', 'rbf'], 'class_weight': [None, 'balanced']}\n",
    "\n",
    "svm_cv = RandomizedSearchCV(estimator=SVC(), param_distributions=param_dist, scoring='roc_auc', cv=10, n_iter=10, n_jobs=-1, verbose=1, random_state=42)\n",
    "svm_cv.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training\n",
    "svm_full = SVC().set_params(**(svm_cv.best_estimator_.get_params()))\n",
    "svm_full.set_params(probability=True)\n",
    "svm_full.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred, y_score = svm_full.predict(X_test_transformed), svm_full.predict_proba(X_test_transformed)\n",
    "y_score = y_score[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "auc_svm = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81812169312169325"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will train a feed-forward neural network. We will do the following steps in order:\n",
    "\n",
    "1. Load the training and test datasets using DataLoader\n",
    "2. Define a Feedforwad Neural Network\n",
    "3. Define a loss function\n",
    "4. Train the network on the training data\n",
    "5. Test the network on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Loading datasets\n",
    "We will use DataLoader and TensorDataset (from [torch.utils.data](http://pytorch.org/docs/master/data.html#)) for convinience in data handling. You can create your custom dataset class by inheriting Dataset with some required member functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "trainset = TensorDataset(torch.from_numpy(X_train_transformed.astype('float32')), torch.from_numpy(y_train.astype('float32')).view(-1,1))\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = TensorDataset(torch.from_numpy(X_test_transformed.astype('float32')), torch.from_numpy(y_test.astype('float32')).view(-1,1))\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check some training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "[torch.FloatTensor of size 4x9978]\n",
      "\n",
      "\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 0\n",
      "[torch.FloatTensor of size 4x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get some random training samples\n",
    "dataiter = iter(trainloader)\n",
    "records, labels = dataiter.next()\n",
    "\n",
    "print(records)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Define a Feed-forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FeedForwardNet(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        super(FeedForwardNet, self).__init__()\n",
    "        self.hidden1 = nn.Linear(n_input, n_hidden)\n",
    "        self.hidden2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.out = nn.Linear(n_hidden, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "net = FeedForwardNet(n_input=9978, n_hidden=256, n_output=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define a Loss function and Optimizer\n",
    "We will use Binary Cross Entropy loss and SGD with momentum as our optimizer.\n",
    "PyTorch provide BCEWithLogitsLoss loss function which combines a Sigmoid layer and the BCEloss together and it is more numerically stable than using them separately. Keep in mind that you should not apply sigmoid activation after the output layer to use this combined loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 0.522\n",
      "[1,    20] loss: 0.549\n",
      "[1,    30] loss: 0.501\n",
      "[1,    40] loss: 0.479\n",
      "[1,    50] loss: 0.474\n",
      "[1,    60] loss: 0.566\n",
      "[2,    10] loss: 0.525\n",
      "[2,    20] loss: 0.474\n",
      "[2,    30] loss: 0.563\n",
      "[2,    40] loss: 0.548\n",
      "[2,    50] loss: 0.456\n",
      "[2,    60] loss: 0.415\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        \n",
    "        if i % 10 == 9:    # print every 10 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 10))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
